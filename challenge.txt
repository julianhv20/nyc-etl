Prueba Técnica Ingeniero de Datos 
Objetivo 
Desarrollar un pipeline ETL que extraiga, transforme y cargue datos de taxis amarillos de NYC a partir de datos públicos, aplicando un patrón de arquitectura de medallón. La solución deberá incluir la capa Raw (datos en crudo), la capa Trusted (datos limpios y enriquecidos) y la capa Refined (agregaciones y KPIs para análisis). Además, el pipeline deberá incorporar mecanismos de observabilidad y gestión de errores. 
Contexto del Dataset 
El dataset de taxis amarillos de NYC (Yellow Taxi Trips) es una fuente de datos abierta que contiene registros históricos de viajes en taxi en la ciudad de Nueva York. Cada registro incluye información de fechas y horas de inicio y fin, ubicaciones (a través de LocationID), distancia, tarifa, número de pasajeros, entre otros. Este dataset es publicado por la NYC Taxi and Limousine Commission (TLC) y se encuentra disponible también a través de plataformas como: 
● AWS S3 (Open Data): Los archivos Parquet están en el bucket público s3://nyc-tlc (https://registry.opendata.aws/nyc-tlc-trip-records-pds/#:~:text=,1). 
● Google BigQuery: El dataset bigquery-public-data.new_york_taxi_trips contiene datos equivalentes 
(https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tlc-trips?hl=es-419). ● Azure Open Datasets: Se puede optar por la versión disponible en Azure (aunque generalmente abarca datos hasta 2018) 
(https://learn.microsoft.com/en-us/azure/open-datasets/dataset-taxi-yellow?tabs=pyspark). ● Fuente original: Otra opción disponible se encuentra en la fuente original de los datos, donde están disponibles en formato PARQUET 
(https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). 
Instrucciones 
La solución deberá estructurarse en tres capas: 
1. Capa Raw 
● Conectar a la fuente de datos elegida (AWS S3, BigQuery, Azure o Archivos planos) y extraer los archivos de uno o más años representativos. 
● Leer los datos en su formato nativo. 
2. Capa Trusted (Clean & Enrich)
Limpieza y Validación 
● Verificar que los timestamps de pickup_datetime sean menores que los de dropoff_datetime. 
● Validar que campos numéricos como trip_distance y fare_amount sean mayores a cero. ● Detectar y manejar outliers (por ejemplo, viajes con distancias o tarifas absurdamente elevadas). Gestionar datos faltantes y valores nulos, asignando defaults o descartando registros que no cumplan reglas mínimas de calidad. 
Homologación y Estandarización 
● Asegurar la consistencia en nombres y tipos de columnas (por ejemplo, unificar tpep_pickup_datetime y pickup_datetime). 
Enriquecimiento 
● Realizar un join con la tabla de Taxi Zone Lookup para traducir el PULocationID a información geográfica (nombre de zona, borough, etc.). 
Recurso: TLC Trip Record Data – Taxi Zone Lookup (Taxi Zone Lookup Table (CSV)). 3. Capa Refined (Agregaciones y KPIs) 
El candidato tendrá la libertad de definir los KPIs que considere más relevantes, pero se deberán responder los siguientes tres requerimientos de negocio de manera que se evidencie la complejidad de la transformación relacional y la integración de múltiples dimensiones: 
Patrón de Demanda y Tiempos Pico 
● Requerimiento: Determinar y comparar la demanda de viajes en distintas franjas horarias y días de la semana. 
● KPIs sugeridos: Variación del número de viajes, duración promedio y tarifas en diferentes franjas (por ejemplo, hora pico vs. hora valle), identificando picos o caídas significativas de demanda que puedan indicar cuellos de botella o cambios operativos. 
Eficiencia Geográfica y Económica 
● Requerimiento: Analizar la eficiencia operativa y económica por zona o borough. KPIs sugeridos: Ingreso promedio por milla/minuto, velocidad promedio (relación de distancia y duración), y ranking de zonas en términos de rentabilidad y eficiencia operacional. Se debe identificar qué áreas son más rentables y dónde se presentan mayores ineficiencias. 
Impacto de la Calidad de Datos 
● Requerimiento: Medir el impacto de la calidad de los datos en los resultados de negocio. ● KPIs sugeridos: Porcentaje de registros descartados o corregidos durante el proceso de transformación, y su efecto en las métricas finales (por ejemplo, comparando ingresos totales
calculados con y sin registros filtrados). Esto servirá para evaluar la robustez del pipeline y la fiabilidad de la fuente de datos. 
La generación de estos KPIs deberá realizarse sobre la capa Refined, utilizando la información ya limpia y enriquecida de la etapa Trusted. 
Implementación de Observabilidad y Gestión de Errores 
● Logging: Incluir logs (niveles INFO, WARNING y ERROR) que indiquen: 
○ Número de registros extraídos, transformados y cargados. 
○ Registros filtrados o descartados debido a validaciones fallidas. 
● Manejo de errores: 
○ Utilizar bloques try/except en partes críticas del pipeline (extracción, transformación y carga) para evitar la terminación abrupta del proceso. 
Documentar la estrategia de manejo de errores y, de ser posible, implementar reintentos automáticos. 
● Reporte de ejecución: Al finalizar el pipeline, generar un reporte (archivo en formato JSON o texto) que resuma: 
○ Total de registros procesados. 
○ Cantidad de registros descartados o corregidos. 
○ Tiempo de ejecución por cada etapa (Extract, Transform, Load). 
○ KPIs calculados. 
Requerimientos Técnicos y Tecnologías 
● Lenguaje y framework: Se debe implementar la aplicación usando un framework de procesamiento distribuido, dado el volumen de datos. Se recomienda el uso de la API de Spark en Python. 
● Organización del código: La solución debe estar estructurada en módulos o scripts que reflejen las etapas del pipeline. Esta estructura debe seguir las mejores prácticas de programación, patrones de diseño y SOLID si aplica. 
● Control de versiones: La solución se entregará en un repositorio público de GitHub con un README detallado. 
● Documentación: Toda la documentación (incluyendo instrucciones de ejecución y descripción del pipeline) debe estar en archivos Markdown (.md). 
● Implementación en la nube (opcional): Se valorará la implementación o integración en alguna nube (AWS, GCP o Azure), describiendo el proceso y la configuración de credenciales, junto a scripts de despliegue (por ejemplo, Terraform). 
Entregables 
1. Código Fuente: Todos los scripts o notebooks que implementen el pipeline ETL (Raw, Trusted y Refined) y el cálculo de KPIs.
2. Repositorio Público en GitHub: El repositorio debe ser público y contener la solución completa. 3. Documentación Técnica: 
○ Archivo README.md con instrucciones de instalación, ejecución y descripción de la arquitectura. 
○ Documentación adicional en formato Markdown explicando decisiones técnicas, reglas de transformación y estrategias de observabilidad. 
4. Reporte de Ejecución: Un log o reporte final que resuma la ejecución del pipeline (registros procesados, errores, tiempos, KPIs). 
5. (Opcional) Pruebas Unitarias o de Integración: Casos de prueba que validen la correcta ejecución de las principales funciones del pipeline. 
Nota: Todos los entregables se deben subir al repositorio de GitHub.