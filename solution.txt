Solution Overview

The solution will be a PySpark application structured into distinct modules corresponding to the ETL stages and layers of the Medallion Architecture.

Source: Publicly available NYC TLC data (e.g., AWS S3 Parquet files).
Raw Layer: Reads data directly from the source, stores it as is.
Trusted Layer: Reads from Raw, applies cleaning, validation, standardization, and enrichment (joining with Zone Lookup).
Refined Layer: Reads from Trusted, performs aggregations and calculates the defined KPIs.
Orchestration: A main script/entry point to manage the flow between layers.
Observability & Error Handling: Integrated logging and try...except blocks throughout the pipeline, culminating in a summary report.
Technical Stack

Language: Python
Processing Framework: Apache Spark (PySpark API)
Storage: Local filesystem or cloud storage (e.g., S3, ADLS Gen2) for Raw, Trusted, Refined layers.
Versioning: Git (GitHub)
Documentation: Markdown
Code Structure (Modular)

├── main.py             # Pipeline entry point and orchestration
├── config.py           # Configuration (file paths, S3 buckets, years, etc.)
├── data_reader.py      # Handles data extraction from source
├── raw_layer.py        # Logic for Raw layer processing (mostly loading)
├── trusted_layer.py    # Logic for Trusted layer (cleaning, enriching)
├── refined_layer.py    # Logic for Refined layer (aggregations, KPIs)
├── utils.py            # Helper functions (logging setup, maybe data quality checks)
├── requirements.txt    # Python dependencies
├── README.md           # Project description, setup, execution
├── docs/               # Additional documentation
│   ├── architecture.md
│   ├── transformations.md
│   └── observability.md
└── data/               # Local data storage (optional, or specify cloud paths in config)
    ├── raw/
    ├── trusted/
    └── refined/
Step-by-Step Implementation Plan

1. Setup and Configuration (requirements.txt, config.py, utils.py)

Create a requirements.txt file: pyspark, pandas (optional, for small lookup table), pyarrow (Spark dependency for Parquet).
Create config.py: Define variables for input source path (e.g., S3 bucket and prefix), years to process, output base path for each layer (data/raw, data/trusted, data/refined), path to the taxi zone lookup CSV.
Create utils.py: Set up a basic Python logging configuration. Function to get a SparkSession.
2. Data Extraction (data_reader.py)

Create data_reader.py.
Implement a function read_taxi_data(spark, source_path, years):
Takes SparkSession, source path pattern (e.g., s3://nyc-tlc/yellow/year=*/month=*), and a list of years as input.
Construct the specific paths for the requested years.
Use spark.read.parquet() to read the data. Spark can read directly from S3 if configured correctly (requires AWS credentials setup in the environment or Spark config).
Log the number of files/partitions found and starting the read.
Return the raw DataFrame.
Include try...except for reading errors.
3. Capa Raw (raw_layer.py)

Create raw_layer.py.
Implement a function process_raw_layer(spark, source_path, output_path, years):
Calls read_taxi_data to get the raw DataFrame.
Log the count of records read.
Write the DataFrame to the specified output path (e.g., data/raw/yellow_taxi/) in Parquet format. Use partitioning if desired (e.g., by year and month, which is often how the source is structured anyway). df.write.parquet(output_path, mode="overwrite", partitionBy=["year", "month"]).
Log the count of records written.
Return the path to the raw data location for the next layer.
Include try...except for writing errors.
4. Capa Trusted (trusted_layer.py)

Create trusted_layer.py.
Implement a function process_trusted_layer(spark, raw_data_path, output_path, lookup_csv_path):
Read the raw data from raw_data_path: spark.read.parquet(raw_data_path).
Log the number of records read from Raw.
Data Quality & Validation:
Perform checks using Spark DataFrame filtering and column expressions:
df = df.filter("tpep_pickup_datetime < tpep_dropoff_datetime") (or whichever columns are present)
df = df.filter("trip_distance > 0")
df = df.filter("fare_amount > 0")
Outliers: Define simple thresholds initially (e.g., trip_distance < 1000, fare_amount < 5000). Document these choices. df = df.filter("trip_distance < 1000 AND fare_amount < 5000"). More advanced outlier detection could be explored if time permits.
Nulls: Decide strategy. For critical fields (pickup_datetime, dropoff_datetime, trip_distance, fare_amount, PULocationID, DOLocationID), dropping rows might be appropriate: df = df.na.drop(subset=["tpep_pickup_datetime", "tpep_dropoff_datetime", "trip_distance", "fare_amount", "PULocationID", "DOLocationID"]). Log how many rows were dropped at each step. Calculate the number of records before and after filtering.
Standardization:
Ensure consistent timestamp types.
Rename columns if necessary (e.g., if one year uses tpep_pickup_datetime and another pickup_datetime, use withColumnRenamed or select specific columns with aliases). Aim for a standard schema.
Enrichment (Join):
Read the Taxi Zone Lookup CSV: df_lookup = spark.read.csv(lookup_csv_path, header=True, inferSchema=True). Ensure LocationID is integer type.
Join the taxi data with the lookup table on pickup and dropoff location IDs. This typically requires two joins or a union of joined data.
Rename lookup columns for clarity before joining (e.g., LocationID to PULocationID, Zone to PUZone, Borough to PUBorough).
df_trusted = df.join(df_lookup.withColumnRenamed("LocationID", "PULocationID").withColumnRenamed("Zone", "PUZone").withColumnRenamed("Borough", "PUBorough"), on="PULocationID", how="left")
df_trusted = df_trusted.join(df_lookup.withColumnRenamed("LocationID", "DOLocationID").withColumnRenamed("Zone", "DOZone").withColumnRenamed("Borough", "DOBorough"), on="DOLocationID", how="left")
Log the number of records after enrichment.
Write the cleaned and enriched DataFrame to the Trusted layer output path in Parquet format, potentially partitioned.
Log the number of records written to Trusted.
Return the path to the trusted data location.
Include try...except for any step (reading, cleaning, joining, writing).
5. Capa Refined (refined_layer.py)

Create refined_layer.py.
Implement a function process_refined_layer(spark, trusted_data_path, output_base_path):
Read the trusted data: df_trusted = spark.read.parquet(trusted_data_path).
Log the number of records read from Trusted.
Prepare for KPIs:
Calculate trip duration: df = df_trusted.withColumn("duration_seconds", (col("tpep_dropoff_datetime").cast("long") - col("tpep_pickup_datetime").cast("long"))). Handle potential negative durations (filter or set to 0).
Extract time features: hour, day of week, day of month, month, year from timestamps using Spark SQL functions (hour, dayofweek, dayofmonth, month, year).
Calculate KPIs: Perform aggregations. Store each KPI result in a separate DataFrame or save them as separate files/tables in the Refined layer.
Patrón de Demanda y Tiempos Pico:
Trips by Hour/Day of Week:
Python

from pyspark.sql.functions import count, avg, hour, dayofweek
demand_kpi = df.groupBy(hour("tpep_pickup_datetime").alias("pickup_hour"), dayofweek("tpep_pickup_datetime").alias("pickup_day_of_week")) \
               .agg(count("*").alias("trip_count"),
                    avg("duration_seconds").alias("avg_duration_seconds"),
                    avg("fare_amount").alias("avg_fare_amount"))
# Save demand_kpi to refined_layer/demand_kpi/
Eficiencia Geográfica y Económica:
Efficiency by Zone/Borough: Handle trip_distance and duration_seconds potentially being zero before division.
Python

from pyspark.sql.functions import sum, when, col
efficiency_kpi = df.filter(col("trip_distance") > 0).filter(col("duration_seconds") > 0) \
                   .groupBy("PUBorough", "PUZone") \
                   .agg((sum("fare_amount") / sum("trip_distance")).alias("avg_fare_per_mile"),
                        (sum("trip_distance") / sum("duration_seconds") * (3600/1609.34)).alias("avg_speed_mph")) # Convert m/s to mph
                   # Optional: Add ranking using window functions if needed
# Save efficiency_kpi to refined_layer/efficiency_kpi/
Impacto de la Calidad de Datos:
This KPI is primarily derived from metrics captured during the Trusted layer processing and reported at the end. However, you can calculate aggregate metrics on the final Trusted data (e.g., Total Revenue) and compare it in the final report to a hypothetical "raw" revenue (which you could calculate by summing fare_amount on the Raw data before any filtering).
Calculate Total Revenue on Trusted data:
Python

total_trusted_revenue = df_trusted.agg(sum("fare_amount").alias("total_revenue")).collect()[0]["total_revenue"]
# This value will be included in the final report.
# You would need to capture the raw total revenue earlier for comparison.
Write the resulting KPI DataFrames to the Refined layer output path (e.g., data/refined/demand_kpi/, data/refined/efficiency_kpi/) in Parquet or another suitable format.
Include try...except for aggregation or writing errors.
Return a dictionary or object containing the calculated scalar KPIs (like total revenue) or paths to the saved KPI dataframes.
6. Orchestration and Reporting (main.py)

Create main.py.
This script will:
Set up logging.
Initialize SparkSession.
Define input parameters (years to process, paths) potentially reading from config.py or command-line arguments.
Record start time.
Call process_raw_layer. Record execution time. Handle potential errors. Log success or failure. Capture record counts.
Call process_trusted_layer. Record execution time. Handle potential errors. Log success or failure. Capture record counts processed and discarded/corrected.
Call process_refined_layer. Record execution time. Handle potential errors. Log success or failure. Capture final KPI values or paths.
Record end time and total execution time.
Generate Report: Create a dictionary summarizing the run:
Start/End Timestamps
Total execution time
Execution time per stage (Raw, Trusted, Refined)
Total records read (Raw)
Records processed by Trusted
Records discarded/corrected by Trusted (calculate as Raw read count - Trusted processed count)
Total records processed by Refined (same as Trusted processed count)
The calculated KPI values (either the aggregated dataframes' locations or scalar values like total revenue).
Save this dictionary to a JSON file (e.g., run_report_<timestamp>.json).
Stop SparkSession.
Include a main try...except block to catch any unhandled exceptions and log a critical error before exiting.
7. Observability and Error Management Refinement

Logging: Ensure detailed logs are placed at key points:
Pipeline start/end.
Start/end of each layer process.
Number of records read from source.
Number of records written to each layer.
Number of records filtered/discarded at each specific validation step in the Trusted layer (e.g., "X records discarded due to invalid timestamps", "Y records discarded due to non-positive distance").
Errors encountered with traceback.
Error Handling: Use try...except blocks around I/O operations (reading/writing) and computationally intensive/risky transformations. Decide on the strategy within the except block:
Log the error at ERROR level.
Potentially raise the exception (raise) if the error is unrecoverable and should stop the pipeline (e.g., source file not found).
Potentially continue processing if the error is non-critical (less likely in this pipeline).
Retries: For a test, simply mentioning where you would add retries is sufficient (e.g., around the initial data reading from a potentially flaky external source like an API, though S3 is usually reliable). Implementing simple retries can use a loop and time.sleep. Advanced retries might use a library or a workflow orchestrator feature (Airflow, etc. - beyond Spark itself).
8. Documentation (README.md, docs/)

README.md:
Project Title and Description.
Architecture Overview (Medallion).
Setup Instructions (Prerequisites: Python, Java, Spark; how to install dependencies pip install -r requirements.txt).
Configuration (Explain config.py and how to set source/output paths, years). Mention AWS credentials if using S3.
Execution Instructions (python main.py).
Description of Output (Where data is stored, where the report is).
Link to additional documentation in docs/.
docs/architecture.md: Detail the Medallion layers and the data flow.
docs/transformations.md: Explain the specific cleaning rules, validation checks, outlier handling strategy, and the join logic used in the Trusted layer. Describe the calculations for each KPI in the Refined layer.
docs/observability.md: Detail the logging strategy, error handling approach, and the format/content of the execution report.
9. GitHub Repository

Create a public GitHub repository.
Commit all code (.py files), requirements (requirements.txt), documentation (.md files), and potentially a small sample of the taxi zone lookup CSV (if not easily downloadable).
Ensure the .gitignore file excludes generated data directories, Python environment folders, etc.
10. Optional Cloud Implementation / Integration

Choose one cloud provider (AWS, GCP, or Azure).
AWS:
Describe using EMR (Elastic MapReduce) or AWS Glue for running the PySpark job.
Explain storing data in S3 buckets.
Mention managing credentials (IAM roles for EC2/EMR instances, or configure Spark with access keys - though IAM roles are preferred).
Discuss using Terraform or CloudFormation to define the infrastructure (e.g., S3 buckets, EMR cluster). Provide example scripts if possible.
GCP:
Describe using Dataproc for Spark jobs.
Explain storing data in Google Cloud Storage (GCS).
Mention managing credentials (Service Accounts).
Discuss using Terraform or Deployment Manager.
Azure:
Describe using Azure Synapse Analytics Spark pools or Azure Databricks.
Explain storing data in ADLS Gen2 (Azure Data Lake Storage).
Mention managing credentials (Service Principals).
Discuss using Terraform or ARM templates.
Update config.py to use cloud storage paths (s3://..., gs://..., abfss://...).
Update README.md and docs/ to explain the cloud setup and execution.
11. Optional Unit/Integration Tests

Use Python's unittest or pytest.
Unit Tests: Test individual functions in utils.py or specific small transformation functions if you break them out. Mock Spark DataFrames for inputs.
Integration Tests: Test the logic within a layer function (e.g., test process_trusted_layer with a small, controlled input DataFrame created manually or from a small test file, and assert the output DataFrame has the expected rows and schema after cleaning/joining). Testing the full pipeline end-to-end might be complex without a dedicated test environment. Focus on critical transformation logic.
Key Considerations and Best Practices

Data Volume: Spark is essential. Ensure operations are distributed (avoid .toPandas() on large DataFrames).
Schema Enforcement: Spark infers schema, but explicitly defining schemas for critical dataframes can make the pipeline more robust.
Partitioning: Use partitioning (e.g., by year/month) when writing Parquet data in Raw and Trusted layers for performance benefits when reading subsets.
Error Handling Granularity: Decide how fine-grained your try...except blocks will be. Wrapping the whole layer function is a minimum; wrapping specific risky operations (like reads from external sources or complex joins) is better.
Outlier Definition: Clearly document the criteria used for outlier detection.
KPI Calculation: Ensure the logic directly addresses the business requirements. Handle edge cases (like zero distance/duration in speed calculation).
Documentation: Be clear and concise. A good README is crucial for someone evaluating the submission.
Code Style: Follow PEP 8 standards.
Dependency Management: Use requirements.txt.
Test Data: For local development, consider downloading a small subset of the data (e.g., one month) to speed up testing cycles.
This detailed plan provides a solid framework for building the required ETL pipeline and demonstrating your data engineering skills according to the technical test specifications. Remember to document your choices and reasoning throughout the process. Good luck!